---
title: "High Certainty for Low numbers of initial conditions - checking robustness(simple1dsolve)"
author: "Joel Eliason"
date: "4/9/2018"
output: html_document
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE,fig.width=12, fig.height=6)
library(deSolve)
library(BMS)
library(BAS)
library(gridExtra)
library(grid)
library(ggplot2)
source("simple1dsolve.R")
source("cubic2dsolve.R")
source("plot_cubic.R")
source("bms_summary.R")
# set.seed(123)
```

We can see that we have quite high certainty for IC=10, and collecting until tf=3.
```{r, warning=FALSE}
step=.01
sd=0.1
degree=4

ics=10
tf=3
times=seq(0,tf,by=step)

numSeeds=200
post_probs<-rep(0,numSeeds)
for (i in 1:numSeeds){
  set.seed(i)
  df<-simple1dsolve(x0=ics,step=step,tf=tf,sd=sd,degree=degree)
  fit<-bas.lm(y1~.,data=df)
  w<-fit$which
  lv=sapply(w,function(z) all(unlist(z)==c(0,1,3,4)))
  idx=min(which(lv == TRUE))
  post_probs[i]=fit$postprobs[idx]
}
hist(post_probs,breaks=40)
median(post_probs)
```

Let's try the same thing for ic=0.0001:
```{r, warning=FALSE}
step=.01
sd=0.1
degree=4

ics=0.0001
tf=3
times=seq(0,tf,by=step)

numSeeds=200
post_probs<-rep(0,numSeeds)
for (i in 1:numSeeds){
  set.seed(i)
  df<-simple1dsolve(x0=ics,step=step,tf=tf,sd=sd,degree=degree)
  fit<-bas.lm(y1~.,data=df)
  w<-fit$which
  lv=sapply(w,function(z) all(unlist(z)==c(0,1,3,4)))
  idx=min(which(lv == TRUE))
  post_probs[i]=fit$postprobs[idx]
}
hist(post_probs,breaks=40)
median(post_probs)
```

Definitely a lot worse. What is going on here? Are we back to that old hypothesis, that we need to cover more of the "phase space"? In view of the polynomial regression OED stuff, I think that might actually be the case.

Let's try this for a range of ICs, and see if this is actually the case:

```{r, warning=FALSE}
step=.001
sd=0.1
degree=4

ics=seq(0.0001,12,length.out=10)
tf=6
times=seq(0,tf,by=step)

numSeeds=200

df_postprobs<-data.frame(matrix(nrow=numSeeds,ncol=1))
for(j in ics){
  post_probs<-rep(0,numSeeds)
  for (i in 1:numSeeds){
    set.seed(i)
    df<-simple1dsolve(x0=j,step=step,tf=tf,sd=sd,degree=degree)
    fit<-bas.lm(y1~.,data=df)
    w<-fit$which
    lv=sapply(w,function(z) all(unlist(z)==c(0,1,3,4)))
    idx=min(which(lv == TRUE))
    post_probs[i]=fit$postprobs[idx]
  }
  df_postprobs=cbind(df_postprobs,post_probs)
}
# df_postprobs
meds<-apply(df_postprobs,2,median)
meds
```

This is a lot more robust. OK, so we can see that the larger our IC is, the better. Also, by changing tf, we can see that staying too long on the attractor doesn't really help us all that much, which is what we would have expected.

However, values above the attracting point may be quite unphysical and hard to experiment with. What do we do in this case?

Should we just sample many ICs below the attracting point for long times? Will that actually help? In many of these cases, we just have the exact same values - thus, they may be of quite limited value. I'd like to try some OED things next.


Check no noise case - theory on ill-conditioning

OED for different geometries

Distributions for parameters for top 2-5 models as you vary noise or sampling - do models leave the 99% probability mass area

