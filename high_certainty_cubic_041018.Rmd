---
title: "high_certainty_cubic_041018"
author: "Joel Eliason"
date: "4/10/2018"
output: html_document
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE,fig.width=12, fig.height=6)
library(deSolve)
library(BMS)
library(BAS)
library(gridExtra)
library(grid)
library(ggplot2)
source("simple1dsolve.R")
source("cubic2dsolve.R")
source("plot_cubic.R")
source("bms_summary.R")
# set.seed(123)
```

Let's check first with IC=(1,1) and tf=2, fitting on $x_{1}$.
```{r, warning=FALSE}
step=.01
sd=0.1
degree=3

ics=c(1,1)
tf=2
times=seq(0,tf,by=step)

numSeeds=200
post_probs<-rep(0,numSeeds)
for (i in 1:numSeeds){
  set.seed(i)
  df<-cubic2dsolve(x0=ics,step=step,tf=tf,sd=sd,degree=degree)
  df$y2=NULL # can also regress against y2, so change y2 to y1 and change y1 to y2 in line below
  fit<-bas.lm(y1~.,data=df)
  w<-fit$which
  lv=sapply(w,function(z) all(unlist(z)==c(0,3,9)))
  idx=min(which(lv == TRUE))
  post_probs[i]=fit$postprobs[idx]
}
hist(post_probs,breaks=40)
median(post_probs)
```

Also, around tf=1.5 is the critical point where the posterior probabilities tend to start to skew much higher.

Let's try the same thing for IC=(-1,1):

```{r, warning=FALSE}
step=.01
sd=0.1
degree=3

ics=c(-1,1)
tf=2
times=seq(0,tf,by=step)

numSeeds=200
post_probs<-rep(0,numSeeds)
for (i in 1:numSeeds){
  set.seed(i)
  df<-cubic2dsolve(x0=ics,step=step,tf=tf,sd=sd,degree=degree)
  df$y2=NULL # can also regress against y2, so change y2 to y1 and change y1 to y2 in line below
  fit<-bas.lm(y1~.,data=df)
  w<-fit$which
  lv=sapply(w,function(z) all(unlist(z)==c(0,3,9)))
  idx=min(which(lv == TRUE))
  post_probs[i]=fit$postprobs[idx]
}
hist(post_probs,breaks=40)
median(post_probs)
```

Same critical point here.

Next, IC=(1,-1):

```{r, warning=FALSE}
step=.01
sd=0.1
degree=3

ics=c(1,-1)
tf=1.6
times=seq(0,tf,by=step)

numSeeds=200
post_probs<-rep(0,numSeeds)
for (i in 1:numSeeds){
  set.seed(i)
  df<-cubic2dsolve(x0=ics,step=step,tf=tf,sd=sd,degree=degree)
  df$y2=NULL # can also regress against y2, so change y2 to y1 and change y1 to y2 in line below
  fit<-bas.lm(y1~.,data=df)
  w<-fit$which
  lv=sapply(w,function(z) all(unlist(z)==c(0,3,9)))
  idx=min(which(lv == TRUE))
  post_probs[i]=fit$postprobs[idx]
}
hist(post_probs,breaks=40)
median(post_probs)
```

Same critical point.

Lastly, check IC=(-1,-1):
```{r, warning=FALSE}
step=.01
sd=0.1
degree=3

ics=c(-1,-1)
tf=2
times=seq(0,tf,by=step)

numSeeds=200
post_probs<-rep(0,numSeeds)
for (i in 1:numSeeds){
  set.seed(i)
  df<-cubic2dsolve(x0=ics,step=step,tf=tf,sd=sd,degree=degree)
  df$y2=NULL # can also regress against y2, so change y2 to y1 and change y1 to y2 in line below
  fit<-bas.lm(y1~.,data=df)
  w<-fit$which
  lv=sapply(w,function(z) all(unlist(z)==c(0,3,9)))
  idx=min(which(lv == TRUE))
  post_probs[i]=fit$postprobs[idx]
}
hist(post_probs,breaks=40)
median(post_probs)
```

Again, very similar results - quite high reconstruction for tf=2, given these noise levels and sampling rates.

Let's try thinning out the samples (by half), but running it to the same tf=2:

```{r, warning=FALSE}
step=.01
sd=0.1
degree=3

ics=c(1,1)
tf=2
times=seq(0,tf,by=step)
thinning=2


numSeeds=200
post_probs<-rep(0,numSeeds)
for (i in 1:numSeeds){
  set.seed(i)
  df<-cubic2dsolve(x0=ics,step=step,tf=tf,sd=sd,degree=degree)
  df<-df[seq(1,nrow(df),by=thinning),]
  df$y2=NULL # can also regress against y2, so change y2 to y1 and change y1 to y2 in line below
  fit<-bas.lm(y1~.,data=df)
  w<-fit$which
  lv=sapply(w,function(z) all(unlist(z)==c(0,3,9)))
  idx=min(which(lv == TRUE))
  post_probs[i]=fit$postprobs[idx]
}
hist(post_probs,breaks=40)
median(post_probs)
```

And half again:

```{r, warning=FALSE}
step=.01
sd=0.1
degree=3

ics=c(1,1)
tf=2
times=seq(0,tf,by=step)
thinning=4


numSeeds=200
post_probs<-rep(0,numSeeds)
for (i in 1:numSeeds){
  set.seed(i)
  df<-cubic2dsolve(x0=ics,step=step,tf=tf,sd=sd,degree=degree)
  df<-df[seq(1,nrow(df),by=thinning),]
  df$y2=NULL # can also regress against y2, so change y2 to y1 and change y1 to y2 in line below
  fit<-bas.lm(y1~.,data=df)
  w<-fit$which
  lv=sapply(w,function(z) all(unlist(z)==c(0,3,9)))
  idx=min(which(lv == TRUE))
  post_probs[i]=fit$postprobs[idx]
}
hist(post_probs,breaks=40)
median(post_probs)
```

Certainly reduced - so there is something important in the density of points, and not just the coverage.

Let's bump the noise level to sd=0.5 in our original example:

```{r, warning=FALSE}
step=.01
sd=0.5
degree=3

ics=c(1,1)
tf=2
times=seq(0,tf,by=step)
thinning=1


numSeeds=200
post_probs<-rep(0,numSeeds)
for (i in 1:numSeeds){
  set.seed(i)
  df<-cubic2dsolve(x0=ics,step=step,tf=tf,sd=sd,degree=degree)
  df<-df[seq(1,nrow(df),by=thinning),]
  df$y2=NULL # can also regress against y2, so change y2 to y1 and change y1 to y2 in line below
  fit<-bas.lm(y1~.,data=df)
  w<-fit$which
  lv=sapply(w,function(z) all(unlist(z)==c(0,3,9)))
  idx=min(which(lv == TRUE))
  post_probs[i]=fit$postprobs[idx]
}
hist(post_probs,breaks=40)
median(post_probs)
```

Quite bad - let's see if decreasing our step size changes anything:

```{r, warning=FALSE}
step=.001
sd=0.5
degree=3

ics=c(1,1)
tf=2
times=seq(0,tf,by=step)
thinning=1


numSeeds=200
post_probs<-rep(0,numSeeds)
for (i in 1:numSeeds){
  set.seed(i)
  df<-cubic2dsolve(x0=ics,step=step,tf=tf,sd=sd,degree=degree)
  df<-df[seq(1,nrow(df),by=thinning),]
  df$y2=NULL # can also regress against y2, so change y2 to y1 and change y1 to y2 in line below
  fit<-bas.lm(y1~.,data=df)
  w<-fit$which
  lv=sapply(w,function(z) all(unlist(z)==c(0,3,9)))
  idx=min(which(lv == TRUE))
  post_probs[i]=fit$postprobs[idx]
}
hist(post_probs,breaks=40)
median(post_probs)
```

That helps - let's dial back down to step=0.1, and increase to tf=3:
```{r, warning=FALSE}
step=.01
sd=0.5
degree=3

ics=c(1,1)
tf=3
times=seq(0,tf,by=step)
thinning=1


numSeeds=200
post_probs<-rep(0,numSeeds)
for (i in 1:numSeeds){
  set.seed(i)
  df<-cubic2dsolve(x0=ics,step=step,tf=tf,sd=sd,degree=degree)
  df<-df[seq(1,nrow(df),by=thinning),]
  df$y2=NULL # can also regress against y2, so change y2 to y1 and change y1 to y2 in line below
  fit<-bas.lm(y1~.,data=df)
  w<-fit$which
  lv=sapply(w,function(z) all(unlist(z)==c(0,3,9)))
  idx=min(which(lv == TRUE))
  post_probs[i]=fit$postprobs[idx]
}
hist(post_probs,breaks=40)
median(post_probs)
```

Still not great - so if our noise is higher, simply taking samples for longer won't help, we'll have to increase the resolution of our samples.

Next, I'd like to check how noise, thinning of samples, tf and IC affect recovery as we vary a bifurcation parameter for different "canonical" bifurcations (thus changing the geometry of the attractors). In particular, we need to be mindful of how that space will change how the IC may be in various "basins of attraction".